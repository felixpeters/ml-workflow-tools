{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic configuration, put these lines at the top of each notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting configuration (basically just change plot size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns of our data frames\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two distinct datasets which we can use for our fraud detection use case:\n",
    "- Transaction data: this includes information like the target variable (fraud/no fraud) or credit card information\n",
    "- Identity data: this includes information like device and operating system of the buyer\n",
    "\n",
    "We can join both datasets using the variable `TransactionID`. More detailed information can be found on the official [competition page](https://www.kaggle.com/c/ieee-fraud-detection/data).\n",
    "\n",
    "Firstly, let's create a symbolic link to our shared data directory, as well as a directory for storing intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../../shared_data/fraud-detection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two data files inside our shared data directory. Since it is read-only, we need to create a directory for storing intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load this data into data frames, i.e., indexed tables. We can use the `pandas` package for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a good practice to use uppercase names for constants\n",
    "DATA_PATH = 'data/'\n",
    "TMP_PATH = 'tmp/'\n",
    "identity = pd.read_csv(f'{DATA_PATH}train_identity.csv')\n",
    "transaction = pd.read_csv(f'{DATA_PATH}train_transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at the number of columns (i.e., features) and rows (i.e., examples) in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Identity dataset has {identity.shape[0]} rows and {identity.shape[1]} columns.')\n",
    "print(f'Transaction dataset has {transaction.shape[0]} rows and {transaction.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine the two datasets using the `merge` function. Since we are primarily interested in transactions, this will be our main table. Thus, we will simply add identity information to each transaction if available. Conceptually, this is equivalent to performing a _left join_ in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(transaction, identity, on='TransactionID', how='left')\n",
    "\n",
    "print(f'Training dataset has {data.shape[0]} rows and {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a first impression of our dataset, we can use `pandas`' `head` function, which prints out the first `n` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our subsequent steps, it's useful to know the nature of our features. With a tabular dataset like this, we mainly distinguish between *categorical* and *numerical* features. Luckily, Kaggle provides us with more information about how to treat each feature (see image and [forum post](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-607486))\n",
    "\n",
    "<img alt=\"Categorical features of fraud detection dataset\" src=\"img/kaggle_feature_information.png\" style=\"width: 800px\"/>\n",
    "\n",
    "Let's mark all these features as categorical to derive better descriptive statistics later on. Right now, they are marked as numbers or don't have a meaningful type at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Type of target variable: {data.isFraud.dtype}')\n",
    "print(f'Type of feature ProductCD: {data.ProductCD.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['isFraud', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', \n",
    "             'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \n",
    "             'DeviceType', 'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19',\n",
    "             'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
    "             'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in cat_feats:\n",
    "    data[feat] = data[feat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Type of target variable: {data.isFraud.dtype}')\n",
    "print(f'Type of feature ProductCD: {data.ProductCD.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check if the data remained unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take a look at the ratio of numerical and categorical features. We do this by looking at the `dtype` attribute of each column. After collecting these, we can use the `Counter` class from the standard library to derive exact counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dtypes = [data[feat].dtype.name for feat in data.columns]\n",
    "c = Counter(dtypes)\n",
    "type_counts = c.most_common()\n",
    "\n",
    "for count in type_counts:\n",
    "    print(f'Dataset contains {count[1]} features of type {count[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our target variable (as for every categorical variable), we can look at the following aspects to get a better understanding of it:\n",
    "- Number of classes\n",
    "- Descriptive statistics: count & frequencies\n",
    "- Plots: histogram\n",
    "\n",
    "Let's look at all these things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Target variable {data.isFraud.name} has {len(data.isFraud.dtype.categories)} classes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isFraud.dtype.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isFraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isFraud.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset seems to be very imbalanced, i.e., one class is overrepresented. In our case, more than 96% of all transactions were not registered as fraudulent.\n",
    "\n",
    "Alternatively to the above generated statistics, we can look at a histogram to get a first impression of our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isFraud.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our features we can look at univariate statistics, which describe characteristics of one particular feature, and bivariate statistics, which focus on relationships between features. We will start with the former part of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we should look at how much missing data we have. The `isnull` function counts missing values, as well as `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = data.isnull().sum().sort_values(ascending=False) / len(data)\n",
    "missing_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that we should exclude a large percentage of our features, probably everything with more than 20% of missing values (since we have a lot of data). Additionally, we should check whether these numbers are similar for both classes. Let's put this functionality inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_val_perc(df, levels, target=None):\n",
    "    data = df\n",
    "    if target is not None:\n",
    "        data = data.loc[data.isFraud == target]\n",
    "    missing_vals = data.isnull().sum().sort_values(ascending=False) / len(data)\n",
    "    for l in levels:\n",
    "        perc = len(missing_vals.loc[missing_vals > l]) / len(missing_vals)\n",
    "        print('Percentage of features with more than {:.0f}% missing values: {:.1f}%'.format(l * 100, perc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.2, 0.5, 0.8]\n",
    "\n",
    "missing_val_perc(data, levels, target=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_perc(data, levels, target=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages are roughly equal, excluding columns should thus be no problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate statistics & plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw an example of how to apply the `describe` function. We can use this to get an impression of our features as well. Let's look at examples for both categorical and numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.card4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.card4.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.card4.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms for some more interesting (and interpretable) features are shown below. Because some of the categorical variables contain a lot of classes, we will write a helper function that plots only the top ten items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_ten(col):\n",
    "    counts = col.value_counts(normalize=True)\n",
    "    top_ten = counts.head(10)\n",
    "    top_ten.plot(kind='bar', title=col.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**Exercise:** Describe the variables `card6` and `P_emaildomain` using descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` function also works for numerical variables. It will offer us basic statistics like the mean, standard deviation and quantiles of a particular column. Let's look at the purchase amount first, which is stored in the `TransactionAmt` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.TransactionAmt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bins(col, bins, labels):\n",
    "    bins = pd.cut(col, bins, labels=labels).value_counts(normalize=True)\n",
    "    bins.plot(kind='bar', title=col.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "labels = ['0-9', '10-49', '50-99', '100-499', '500-999', '1,000-4,999', '5,000-9,999', '10,000-50,000']\n",
    "\n",
    "plot_bins(data.TransactionAmt, bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could repeat this process for the other numerical variables (and you **should** definitely do that for your own data!), but since this is a Kaggle competition most of the features are anonymized and thus not very interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Describe the variables `dist1` and `dist2` using descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate statistics & plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at some univariate statistics, we can now go on to examine bivariate relationships within our data. Here, we will mainly look at correlation statistics between features (and the target variable) and create some plots that might give us intuitions about relationships within our dataset.\n",
    "\n",
    "Since we have two variable types (categorical & numerical), we have to account for three relationship types:\n",
    "- numerical-numerical\n",
    "- categorical-categorical\n",
    "- categorical-numerical\n",
    "\n",
    "Let's start with the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `TransactionAmt` and `TransactionDT`, since we can interpret these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['TransactionAmt', 'TransactionDT']].dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subtract the minimum from all datetimes to get more meaningful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TransactionDT'] = df['TransactionDT'] - df['TransactionDT'].min()\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the correlation coefficient and create some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sort_values('TransactionDT').sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_sample.TransactionDT, df_sample.TransactionAmt, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is no pattern to exploit for the transaction date, as indicated by the correlation coefficient (0.01) and visual inspection of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Examine the relationship between the variables `TransactionAmt` and `dist1`/`dist2` using the outlined process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two categorical variables, we can create matrices based on value counts and stacked bar charts. As an example, let's look at the relationship between credit card company and fraudulence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fraud_percentage_per_class(data, col, return_df=False):\n",
    "    # extract and aggregate necessary data\n",
    "    counts = data[[col, 'isFraud']].dropna().groupby([col, 'isFraud']).size()\n",
    "    index = []\n",
    "    percs = []\n",
    "    # calculate fraud/no fraud percentages and save in data frame\n",
    "    for level in counts.index.levels[0]:\n",
    "        level_counts = counts[level]\n",
    "        no_fraud = level_counts.iloc[0] / level_counts.sum()\n",
    "        fraud = 0\n",
    "        if len(level_counts) > 1:\n",
    "            fraud = level_counts.iloc[1] / level_counts.sum()\n",
    "        percs.append((fraud, no_fraud))\n",
    "        index.append(level)\n",
    "    df = pd.DataFrame(columns=['fraud', 'no_fraud'], index=index, data=percs)\n",
    "    plt.bar(df.index, df.fraud, width=0.4)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('fraud percentage')\n",
    "    plt.show()\n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = plot_fraud_percentage_per_class(data, 'card4', return_df=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that fraudulent activity is far more common for credit card company \"Discover\". This could be a valuable insight in model building. Let's plot this for some more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Examine fraud percentages for the variables `card6` and `DeviceType`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One numerical, one categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are looking at one numerical and one categorical variable, we can for example examine descriptive statistics per class. Let's write a function that aggregates numerical data and creates a DataFrame containing the mean, median and standard deviation for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_per_class(data, cat, num, return_df=False):\n",
    "    df = data[[cat, num]].groupby(cat).agg(['mean', 'std', 'median'])\n",
    "    df[(num, 'mean')].plot(kind='bar')\n",
    "    plt.show()\n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = plot_mean_per_class(data, 'isFraud', 'TransactionAmt', return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reuse this function to generate some more plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Examine the relationships of the following variable pairs:\n",
    "- `DeviceType` and `TransactionAmt`\n",
    "- `card4` and `TransactionAmt`\n",
    "- `card6` and `TransactionAmt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this part of the ML workflow, let's save our data to a CSV file for later use. We will also look at the file size in order to compare it to the following steps in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'{TMP_PATH}data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh tmp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "280px",
    "width": "309px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
