{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall strategy for feature engineering will include the following steps:\n",
    "1. Apply domain knowledge to drop features that are not interpretable\n",
    "2. Drop features with too many missing values (attribute sampling)\n",
    "3. Drop examples with too many missing values (record sampling)\n",
    "4. Transform numerical features\n",
    "5. Encode categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic configuration, put these lines at the top of each notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting configuration (basically just change plot size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns of our data frames\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'tmp/'\n",
    "raw = pd.read_csv(f'{DATA_PATH}data_raw.csv')\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning & sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying domain knowledge to reduce features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features in our dataset were anonymized and are thus hard to interpret. Luckily, Vesta provides some abstract description of the feature groups in a [Kaggle forum post](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-607486). You can see the most important information below.\n",
    "\n",
    "<img src=\"img/data_description.png\" alt=\"Data description on Kaggle forum\" style=\"width: 800px\" />\n",
    "\n",
    "Accordingly, we will drop the `V`, `C`, `D` and `id` features because there is no way for us to interpret them during the model evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(raw.columns.to_series()[\"V1\":\"V339\"], axis=1)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(raw.columns.to_series()[\"id_01\":\"id_38\"], axis=1)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(raw.columns.to_series()[\"C1\":\"C14\"], axis=1)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(raw.columns.to_series()[\"D1\":\"D15\"], axis=1)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(raw.columns.to_series()[\"M1\":\"M9\"], axis=1)\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preliminary step leaves us with 19 features (including the target variable) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can reuse some code from the previous step to show us which columns have the most missing values. We will then decide how many of these we have to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.2, 0.5, 0.8]\n",
    "missing_val_cols = raw.isnull().sum().sort_values(ascending=False) / len(raw)\n",
    "\n",
    "for l in levels:\n",
    "    perc = len(missing_val_cols.loc[missing_val_cols > l]) / len(missing_val_cols)\n",
    "    print('Percentage of features with more than {:.0f}% missing values: {:.1f}%'.format(l * 100, perc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_cols * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our cutoff at 20% of missing values, i.e., columns with more than 20% of missing values will be dropped. However, we will make an exception for the features `dist1`, `DeviceType` and `R_emaildomain`, since they are interpretable and might be important for predicting fraud. We will also drop two columns that are of no value to use, namely the index column and `TransactionID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.2\n",
    "\n",
    "cols_to_drop = missing_val_cols.loc[missing_val_cols > cutoff].index.to_list()\n",
    "cols_to_drop.remove(\"DeviceType\")\n",
    "cols_to_drop.remove(\"R_emaildomain\")\n",
    "cols_to_drop.remove(\"dist1\")\n",
    "cols_to_drop.append(\"TransactionID\")\n",
    "cols_to_drop.append(\"Unnamed: 0\")\n",
    "len(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of columns before attribute sampling: {raw.shape[1]}')\n",
    "raw = raw.drop(labels=cols_to_drop, axis=1)\n",
    "print(f'Number of columns after attribute sampling: {raw.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 15 features left after accounting for missing values and our domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use a similar process to remove examples with too many missing values. Including these in our analysis might skew the results, because we they contain too many imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.1, 0.2, 0.5]\n",
    "missing_attrs = raw.isnull().sum(axis=1).sort_values(ascending=False) / raw.shape[1]\n",
    "\n",
    "for l in levels:\n",
    "    perc = len(missing_attrs.loc[missing_attrs >= l]) / len(missing_attrs)\n",
    "    print('Percentage of records with more than {:.0f}% missing values: {:.1f}%'.format(l * 100, perc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of data at our hands, we can easily remove all examples with more than 20% of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cutoff = 0.2\n",
    "\n",
    "print(f'Number of rows before record sampling: {len(raw)}')\n",
    "rows_to_drop = missing_attrs.loc[missing_attrs > cutoff].index.to_list()\n",
    "raw = raw.drop(labels=rows_to_drop, axis=0)\n",
    "print(f'Number of rows after record sampling: {len(raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to save our progress. We have to reset our index (remember: we remove rows, thus creating holes in the existing index) in order to store the data frame in the efficient _Feather_ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.to_feather(f'{DATA_PATH}feats_raw.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have missing values left in our dataset. In the following, we will discover different ways of dealing with them. Firstly, let's calculate the percentage of missing values in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals_sum = raw.isnull().sum().sum() \n",
    "print(f'Percentage of missing values: {missing_vals_sum / (raw.shape[0] * raw.shape[1]) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw.head(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with missing values for categorical and numerical variables separately. Let's write a helper function that splits these variable types for us (this function is borrowed from the great [fastai library](https://docs.fast.ai/tabular.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_cat_split(df, dep_var=None):\n",
    "    cont_names, cat_names = [], []\n",
    "    for label in df:\n",
    "        if label == dep_var: continue\n",
    "        if df[label].dtype == int or df[label].dtype == float: cont_names.append(label)\n",
    "        else: cat_names.append(label)\n",
    "    return cont_names, cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars, cat_vars = cont_cat_split(raw, dep_var='isFraud')\n",
    "print(f'Number of numerical variables: {len(num_vars)}')\n",
    "print(f'Number of categorical variables: {len(cat_vars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `SimpleImputer` from the `scikit-learn` package to impute values for our numeric variables. Here, we will apply the `median` strategy, because both `TransactionAmt` and `dist1` are probably skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace missing values for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(missing_values=np.NaN, strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in num_vars:\n",
    "    raw[var] = num_imputer.fit_transform(X=raw[[var]])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace missing values for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our categorical variables have a lot of unique values which slows imputation down a lot. Therefore, we should gather less popular categories which will also our model to make sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.card1 = raw.card1.astype('int64').astype('category')\n",
    "raw.card2 = raw.card2.astype('int64').astype('category')\n",
    "raw.card3 = raw.card3.astype('int64').astype('category')\n",
    "raw.card5 = raw.card5.astype('int64').astype('category')\n",
    "raw.addr1 = raw.addr1.astype('int64').astype('category')\n",
    "raw.addr2 = raw.addr2.astype('int64').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_of_top_n_cats(col, n):\n",
    "    counts = col.value_counts()\n",
    "    total_count = counts.sum()\n",
    "    top_n_count = counts[:n].sum()\n",
    "    print(f'Coverage of top {n} categories for column {col.name}: {top_n_count/total_count*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in cat_vars:\n",
    "    unique_vals = len(pd.unique(raw[var]))\n",
    "    print(f'Unique values in {var}: {unique_vals}')\n",
    "    coverage_of_top_n_cats(raw[var], 10)\n",
    "    coverage_of_top_n_cats(raw[var], 20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 categories describe pretty much most of all our categorical features. We can therefore condense the long tail into one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_numerical_categories(col, n=10):\n",
    "    top_ten_cats = list(col.value_counts().index[:n])\n",
    "    mask = [False if row in top_ten_cats else True for row in col]\n",
    "    temp = col.mask(mask, other=0)\n",
    "    d = {0: (n+1)}\n",
    "    for i, cat in zip(list(range(1, (n+1))), top_ten_cats):\n",
    "        d[cat] = i\n",
    "    return temp.astype('category').cat.rename_categories(d)\n",
    "\n",
    "def restructure_string_categories(col, n=10):\n",
    "    top_ten_cats = list(col.value_counts().index[:n])\n",
    "    mask = [False if row in top_ten_cats else True for row in col]\n",
    "    temp = col.mask(mask, other=\"other\")\n",
    "    return temp.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.card1 = restructure_numerical_categories(raw.card1)\n",
    "raw.card2 = restructure_numerical_categories(raw.card2)\n",
    "raw.card3 = restructure_numerical_categories(raw.card3)\n",
    "raw.card5 = restructure_numerical_categories(raw.card5)\n",
    "raw.addr1 = restructure_numerical_categories(raw.addr1)\n",
    "raw.addr2 = restructure_numerical_categories(raw.addr2)\n",
    "raw.P_emaildomain = restructure_string_categories(raw.P_emaildomain)\n",
    "raw.R_emaildomain = restructure_string_categories(raw.R_emaildomain)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all our large categorical features only contain 11 distinct categories, where the category number also reflects the category's frequency.\n",
    "At this point, we can impute values for missing values in the categorical variables. We will use constants for this (using the most frequent item would manipulate features with lots of missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imputer = SimpleImputer(missing_values=float('nan'), strategy=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in cat_vars:\n",
    "    raw[var] = cat_imputer.fit_transform(X=raw[[var]])\n",
    "    raw[var] = raw[var].astype('category')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw.addr1.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming that we don't have any missing values left, we can save our progress and go on to transformation of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals_sum = raw.isnull().sum().sum() \n",
    "print(f'Percentage of missing values: {missing_vals_sum / (raw.shape[0] * raw.shape[1]) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.to_feather(f'{DATA_PATH}feats_clean.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(f'{DATA_PATH}feats_clean.feather')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by making more sense of our datetime feature. Our goal is to decompose it into day of the week and hour of the day. With our anonymized, relative datetime it is hard to retrieve more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset is used to shift the start/end of a day, experimentation shows that offset of 0.58 is optimal \n",
    "def make_day_feature(col, offset=0):\n",
    "    days = col / (3600*24)        \n",
    "    encoded_days = np.floor(days-1+offset) % 7\n",
    "    return encoded_days\n",
    "\n",
    "def make_hour_feature(col):\n",
    "    hours = col / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day'] = make_day_feature(data['TransactionDT'], offset=0.58).astype('int64').astype('category')\n",
    "data['hour'] = make_hour_feature(data['TransactionDT']).astype('int64').astype('category')\n",
    "print(data.day.describe())\n",
    "print(\"\\n\")\n",
    "print(data.hour.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"TransactionDT\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the new features are added to the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling numeric variables is useful for models that are susceptible to different feature ranges, e.g., logistic regression. We will bring all our numeric variables to the range (0, 1) using the `MinMaxScaler` from the `scikit-learn` package. Beforehand, we will use a log transformation in order to de-skew the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars, cat_vars = cont_cat_split(data, dep_var='isFraud')\n",
    "num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "log_transformer = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=False)\n",
    "\n",
    "for var in num_vars:\n",
    "    data[var] = log_transformer.fit_transform(data[[var]])\n",
    "    data[var] = scaler.fit_transform(data[[var]])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw an example of discretization in our exploratory data analysis, when we binned numerical data for plotting. Discretization does not make sense for our features, but an example is included nonetheless. We will use pandas' `cut` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_amounts = raw.TransactionAmt\n",
    "bins = [0, 10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "\n",
    "pd.cut(transaction_amounts, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data was discretized into eight bins, replacing the original numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have two numeric features left, there is only one possible interaction term to include in our dataset. We will add a `dist*TransactionAmt` feature and examine whether it might be a good predictor for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dist1*TransactionAmt'] = data.dist1 * data.TransactionAmt\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['isFraud', 'dist1*TransactionAmt']].groupby('isFraud').agg(['mean', 'median'])\n",
    "df[('dist1*TransactionAmt', 'mean')].plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction feature will probably not help much, since the means for both groups are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should encode our categorical variables in order to derive meaningful features that are also interpretable. Since our categorical features are non-ordinal, we can use one-hot encoding which will create a new feature for every level in each categorical variable. This will results in a \"wider\" dataset, i.e., a data frame with more columns than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(data[cat_vars], prefix=cat_vars)\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, one_hot_df], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw an example of how to deal with large categorical variables, when we limited several features to their top ten categories and an additional `other` category. Other common approaches include feature hashing or bin counting, which we will not further elaborate on here.\n",
    "An alternative to our approach would be to one-hot encode a feature with many categories and subsequently apply an dimensionality reduction algorithm such as PCA in order to reduce the number of columns. This approach is often used in Kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_feather(f'{DATA_PATH}feats_final.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our file size is down to less than 100MB from the original 700MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "314px",
    "width": "381px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
